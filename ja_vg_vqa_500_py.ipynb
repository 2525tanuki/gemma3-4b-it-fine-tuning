{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4432c459f9924c348e3b083e89dc1466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d905943ad9f64a0eb000863373d84cff",
              "IPY_MODEL_11b0ba4421f8439dbb136a755fe314f5",
              "IPY_MODEL_c3d4cbe2d7254c948da4f8ed8bc9367a"
            ],
            "layout": "IPY_MODEL_991d28ab61c446bbaf6cb49db8f674e0"
          }
        },
        "d905943ad9f64a0eb000863373d84cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d72f493339cc4608831c145ae92feb60",
            "placeholder": "​",
            "style": "IPY_MODEL_f1bbe8e69155487684900f4a8666623b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "11b0ba4421f8439dbb136a755fe314f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4183bcb4ddb4dfaa2c6d69c2af03d0e",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e55e46fb84844ccea4d784b59d83996a",
            "value": 6
          }
        },
        "c3d4cbe2d7254c948da4f8ed8bc9367a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e84d9f488a4523a3ab75feb0d75b28",
            "placeholder": "​",
            "style": "IPY_MODEL_ea07090fd7a444d4bd2127bf35bccbe0",
            "value": " 6/6 [01:31&lt;00:00, 15.02s/it]"
          }
        },
        "991d28ab61c446bbaf6cb49db8f674e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d72f493339cc4608831c145ae92feb60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1bbe8e69155487684900f4a8666623b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4183bcb4ddb4dfaa2c6d69c2af03d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e55e46fb84844ccea4d784b59d83996a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96e84d9f488a4523a3ab75feb0d75b28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea07090fd7a444d4bd2127bf35bccbe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## pip"
      ],
      "metadata": {
        "id": "Wj2jMVK6Q30I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2NChfZPQ9nK",
        "outputId": "3ac86318-9d27-4bf9-8136-4947e7c2a25d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TPzuLjjWw9e",
        "outputId": "f4d9180b-8d22-48cf-b87c-733e6dd8915a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=54c8fb03068cd2c897b18c11f58dda73505c4201d944684b25c348b0035ea5e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi\n",
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knxKGNwLXPbb",
        "outputId": "5ea464dc-6c44-49f3-fcd5-0e2212f33978"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fugashi\n",
            "  Downloading fugashi-1.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Downloading fugashi-1.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (698 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/698.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.0/698.0 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fugashi\n",
            "Successfully installed fugashi-1.4.0\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidic-lite # for MeCab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmgfPkoRoNz6",
        "outputId": "610f67a0-b82a-48db-bf32-4aa0f07960b4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=2ebdc67ddd145b4210a32382fba2bb152538af9444648070d1873c7f3fd7cb04\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/fd/e9/ea4459b868e6d2902e8d80e82dbacb6203e05b3b3a58c64966\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importする"
      ],
      "metadata": {
        "id": "48kN_Z5YQ56b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "from typing import Callable"
      ],
      "metadata": {
        "id": "2Xz0hCATQNVo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass\n",
        "from datasets import Dataset, concatenate_datasets, load_dataset\n",
        "from rouge_score import rouge_scorer, scoring"
      ],
      "metadata": {
        "id": "XIhvCLeLTdW-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ProcessPoolExecutor, Future"
      ],
      "metadata": {
        "id": "rnb2qgSSWPYj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fugashi import Tagger"
      ],
      "metadata": {
        "id": "p5R6320OXIDF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji"
      ],
      "metadata": {
        "id": "ITjSQ9AAXIYa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re"
      ],
      "metadata": {
        "id": "6tbUysQrXKPb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMjpのコードを一部利用"
      ],
      "metadata": {
        "id": "jPeGdIj3RYl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TaskConfig"
      ],
      "metadata": {
        "id": "hymnZrFJTqxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TaskConfig:\n",
        "    max_dataset_len: int | None = None\n",
        "    rotate_choices: bool = False"
      ],
      "metadata": {
        "id": "CHys_3VuTqO5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task"
      ],
      "metadata": {
        "id": "U0Tmd12DTWJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Task(abc.ABC):\n",
        "    def __init__(self, config: TaskConfig):\n",
        "        self.config = config\n",
        "\n",
        "        if self.config.max_dataset_len is not None:\n",
        "            self.dataset = self._prepare_dataset().select(\n",
        "                range(self.config.max_dataset_len)\n",
        "            )\n",
        "        else:\n",
        "            self.dataset = self._prepare_dataset()\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _prepare_dataset(self) -> Dataset:\n",
        "        \"\"\"Prepares the dataset.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def doc_to_text(self, doc) -> str:\n",
        "        \"\"\"Converts a document to text.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def doc_to_visual(self, doc) -> list[Image.Image]:\n",
        "        \"\"\"Converts a document to visual.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def doc_to_id(self, doc) -> str:\n",
        "        \"\"\"Converts a document to id.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def doc_to_answer(self, doc) -> str:\n",
        "        \"\"\"Converts a document to answer.\"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "JPIeX291TYDq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JaVGVQA500"
      ],
      "metadata": {
        "id": "s2f4geaeUPoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class JaVGVQA500(Task):\n",
        "    default_metric = \"rougel\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_dataset() -> Dataset:\n",
        "        ds = load_dataset(\"SakanaAI/JA-VG-VQA-500\", split=\"test\")\n",
        "\n",
        "        def flatten_sample(sample):\n",
        "            dataset = {\n",
        "                \"image_id\": [sample[\"image_id\"] for _ in sample[\"qas\"]],\n",
        "                \"image\": [sample[\"image\"] for _ in sample[\"qas\"]],\n",
        "                \"qa_id\": [qa[\"qa_id\"] for qa in sample[\"qas\"]],\n",
        "                \"question\": [qa[\"question\"] for qa in sample[\"qas\"]],\n",
        "                \"answer\": [qa[\"answer\"] for qa in sample[\"qas\"]],\n",
        "            }\n",
        "            return Dataset.from_dict(dataset)\n",
        "\n",
        "        fragments = []\n",
        "        for i, sample in enumerate(ds):\n",
        "            data_fragment = flatten_sample(sample)\n",
        "            fragments.append(data_fragment)\n",
        "\n",
        "        ds = concatenate_datasets(fragments)\n",
        "        ds = ds.rename_column(\"question\", \"input_text\")\n",
        "        ds = ds.rename_column(\"qa_id\", \"question_id\")\n",
        "\n",
        "        return ds\n",
        "\n",
        "    @staticmethod\n",
        "    def doc_to_text(doc) -> str:\n",
        "        return doc[\"input_text\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def doc_to_visual(doc) -> list[Image.Image]:\n",
        "        return [doc[\"image\"]]\n",
        "\n",
        "    @staticmethod\n",
        "    def doc_to_id(doc) -> str:\n",
        "        return str(doc[\"question_id\"])\n",
        "\n",
        "    @staticmethod\n",
        "    def doc_to_answer(doc) -> str:\n",
        "        return doc[\"answer\"]\n"
      ],
      "metadata": {
        "id": "PiZWh9AdUSuB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TaskRegistry"
      ],
      "metadata": {
        "id": "eHjlZIotSrTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaskRegistry:\n",
        "    \"\"\"Registry to map metrics to their corresponding scorer classes.\"\"\"\n",
        "\n",
        "    _tasks: dict[str, Callable[[TaskConfig], Task]] = {\n",
        "        \"ja-vg-vqa-500\": JaVGVQA500,\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def get_task_list(cls):\n",
        "        return list(cls._tasks.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def load_task(cls, task_name: str, task_config: TaskConfig = TaskConfig()) -> Task:\n",
        "        try:\n",
        "            return cls._tasks[task_name](task_config)  # type: ignore\n",
        "        except KeyError:\n",
        "            raise ValueError(f\"Task '{task_name}' is not supported.\")"
      ],
      "metadata": {
        "id": "8KSDkW3xRbbu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AggregateOutput"
      ],
      "metadata": {
        "id": "i_VxtM6-Vt_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AggregateOutput:\n",
        "    overall_score: float\n",
        "    details: dict[str, float]"
      ],
      "metadata": {
        "id": "wvUWBHr8VwK6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ScorerConfig"
      ],
      "metadata": {
        "id": "xWaFxRDgVUej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ScorerConfig:\n",
        "    docs: dict | None = None\n",
        "    judge_model: str | None = None\n",
        "    batch_size: int = 10\n",
        "    random_choice: bool = False"
      ],
      "metadata": {
        "id": "a-4enCOqVWzh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scorer"
      ],
      "metadata": {
        "id": "4XzQvN8UVf7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Scorer(ABC):\n",
        "    def __init__(self, config: ScorerConfig):\n",
        "        self.config = config\n",
        "\n",
        "    @abstractmethod\n",
        "    def score(self, refs: list[str], preds: list[str]) -> list:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def aggregate(self, scores: list) -> AggregateOutput:\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "AAwtoewZVfev"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MecabTokenizer"
      ],
      "metadata": {
        "id": "FGTQPf71WfZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MecabTokenizer:\n",
        "    def __init__(self) -> None:\n",
        "        self.tagger = Tagger(\"-Owakati\")\n",
        "\n",
        "    def normalize_answer(self, text: str) -> str:\n",
        "        \"\"\"Lower case text, remove punctuation and extra whitespace, etc.\"\"\"\n",
        "\n",
        "        def white_space_fix(text: str) -> str:\n",
        "            return \" \".join(text.split())\n",
        "\n",
        "        def remove_emoji(text: str) -> str:\n",
        "            text = \"\".join([\"\" if emoji.is_emoji(c) else c for c in text])\n",
        "            emoji_pattern = re.compile(\n",
        "                \"[\"\n",
        "                \"\\U0001f600-\\U0001f64f\"  # emoticons\n",
        "                \"\\U0001f300-\\U0001f5ff\"  # symbols & pictographs\n",
        "                \"\\U0001f680-\\U0001f6ff\"  # transport & map symbols\n",
        "                \"\\U0001f1e0-\\U0001f1ff\"  # flags (iOS)\n",
        "                \"\\U00002702-\\U000027b0\"\n",
        "                \"]+\",\n",
        "                flags=re.UNICODE,\n",
        "            )\n",
        "            return emoji_pattern.sub(r\"\", text)\n",
        "\n",
        "        text = remove_emoji(text)\n",
        "        # see neologdn docs for details, but handles things like full/half width variation\n",
        "        # text = neologdn.normalize(text) FIXME: fix c++12 error when installing neologdn\n",
        "        text = unicodedata.normalize(\"NFKC\", text)\n",
        "        text = white_space_fix(text)\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self.tagger.parse(self.normalize_answer(text)).split()"
      ],
      "metadata": {
        "id": "l0WYwliOWen0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### rouge_ja"
      ],
      "metadata": {
        "id": "H9IKSbtFWTtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rouge_ja(refs: list[str], preds: list[str]) -> dict:\n",
        "    \"\"\"Compute ROUGE-L scores for Japanese text.\n",
        "    Args:\n",
        "        refs: list of reference strings\n",
        "        preds: list of predicted strings\n",
        "    Returns:\n",
        "        dict: dictionary with keys: { 'rouge1', 'rouge2', 'rougeL' }\n",
        "        Each value is a float representing the ROUGE score (f-measure) * 100.\n",
        "    \"\"\"\n",
        "    assert isinstance(refs, list) and isinstance(\n",
        "        preds, list\n",
        "    ), \"refs and preds must be lists.\"\n",
        "    tokenizer = MecabTokenizer()\n",
        "    rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
        "    # mecab-based rouge\n",
        "    scorer = rouge_scorer.RougeScorer(\n",
        "        rouge_types,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Accumulate confidence intervals.\n",
        "    aggregator = scoring.BootstrapAggregator()\n",
        "    for ref, pred in zip(refs, preds):\n",
        "        aggregator.add_scores(scorer.score(ref, pred))\n",
        "    result = aggregator.aggregate()\n",
        "    return {type: result[type].mid.fmeasure * 100 for type in rouge_types}\n"
      ],
      "metadata": {
        "id": "buwcdj-TWWx-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RougeLScorer"
      ],
      "metadata": {
        "id": "8xXUA9kgV-p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RougeLScorer(Scorer):\n",
        "    @staticmethod\n",
        "    def score(refs: list[str], preds: list[str]) -> list[float]:\n",
        "        futures: list[Future[dict[str, float]]] = []\n",
        "        with ProcessPoolExecutor() as executor:\n",
        "            for ref, pred in zip(refs, preds):\n",
        "                future = executor.submit(rouge_ja, [ref], [pred])\n",
        "                futures.append(future)\n",
        "        scores = [f.result()[\"rougeL\"] for f in futures]\n",
        "        return scores\n",
        "\n",
        "    @staticmethod\n",
        "    def aggregate(scores: list[float]) -> AggregateOutput:\n",
        "        mean = sum(scores) / len(scores)\n",
        "        return AggregateOutput(mean, {\"rougel\": mean})"
      ],
      "metadata": {
        "id": "5_2PZdsuV_wD"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ScorerRegistry"
      ],
      "metadata": {
        "id": "miTrCymYUxfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScorerRegistry:\n",
        "    \"\"\"Registry to map metrics to their corresponding scorer classes.\"\"\"\n",
        "\n",
        "    _scorers: dict[str, Callable[[ScorerConfig], Scorer]] = {\n",
        "        \"rougel\": RougeLScorer,\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def get_metric_list(cls) -> list[str]:\n",
        "        \"\"\"Get a list of supported metrics.\"\"\"\n",
        "        return list(cls._scorers.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def load_scorer(\n",
        "        cls, metric: str, scorer_config: ScorerConfig = ScorerConfig()\n",
        "    ) -> Scorer:\n",
        "        \"\"\"Load a scorer instance from the scorer registry.\"\"\"\n",
        "        try:\n",
        "            return cls._scorers[metric](scorer_config)  # type: ignore\n",
        "        except KeyError:\n",
        "            raise ValueError(f\"Metric '{metric}' is not supported.\")"
      ],
      "metadata": {
        "id": "ws5-6fj3UygG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemmaのwrapperクラス。LLMjp eval-mmを回すためにwrapperする"
      ],
      "metadata": {
        "id": "hhdzEOiBQmoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. GemmaVLMWrapper クラス: マージ済みモデルをロードして generate() を実装\n",
        "class GemmaVLMWrapper:\n",
        "    def __init__(self, model_dir: str):\n",
        "        self.model = AutoModelForImageTextToText.from_pretrained(\n",
        "            model_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
        "        )\n",
        "        self.processor = AutoProcessor.from_pretrained(model_dir, use_fast=True)\n",
        "\n",
        "    def generate(\n",
        "        self, images: list[Image.Image], text: str, max_new_tokens: int = 50\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        images: PIL.Image のリスト。必ず1枚以上の画像が渡されることを前提とします。\n",
        "        text: 入力テキスト（質問文など）。\n",
        "        max_new_tokens: 生成する最大トークン数。\n",
        "        \"\"\"\n",
        "        # チャット形式で入力プロンプトを作成します。\n",
        "        # collate_fn の中と同様に、ユーザー発話で画像とテキストを組み合わせたメッセージリストを作ります。\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [{\"type\": \"image\", \"image\": img} for img in images]\n",
        "                + [{\"type\": \"text\", \"text\": text}],\n",
        "            }\n",
        "        ]\n",
        "        # ここでチャットテンプレートを適用し、画像トークンが含まれるプロンプト文字列を生成します。\n",
        "        prompt = self.processor.apply_chat_template(\n",
        "            messages, add_generation_prompt=False, tokenize=False\n",
        "        )\n",
        "\n",
        "        # processor に渡す際は、画像はもともとリストのまま（さらにリストでラップ）にする必要があります。\n",
        "        inputs = self.processor(\n",
        "            text=[prompt], images=[images], return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "        return self.processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "BuCEOyz2zROi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarkを回す"
      ],
      "metadata": {
        "id": "QxxCRdTpX-UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. メイン処理: LLMjp のタスク \"japanese-heron-bench\" のサンプルをロードし評価実施\n",
        "\n",
        "  # 1. タスクをロード\n",
        "task = TaskRegistry.load_task(\"ja-vg-vqa-500\")"
      ],
      "metadata": {
        "id": "CE4ovGm5RQF9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemmaモデルのラッパーを初期化（保存済みマージ済みモデルのディレクトリ）\n",
        "model_dir = \"/content/drive/MyDrive/gemma3-jicvqa-finetuned\" # todo; fix model loading path\n",
        "model = GemmaVLMWrapper(model_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4432c459f9924c348e3b083e89dc1466",
            "d905943ad9f64a0eb000863373d84cff",
            "11b0ba4421f8439dbb136a755fe314f5",
            "c3d4cbe2d7254c948da4f8ed8bc9367a",
            "991d28ab61c446bbaf6cb49db8f674e0",
            "d72f493339cc4608831c145ae92feb60",
            "f1bbe8e69155487684900f4a8666623b",
            "f4183bcb4ddb4dfaa2c6d69c2af03d0e",
            "e55e46fb84844ccea4d784b59d83996a",
            "96e84d9f488a4523a3ab75feb0d75b28",
            "ea07090fd7a444d4bd2127bf35bccbe0"
          ]
        },
        "id": "4UzEQK1YRRg1",
        "outputId": "a3d91aa8-beff-4824-b51c-6ccb2bd9be1f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4432c459f9924c348e3b083e89dc1466"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. タスクのすべてのサンプルをループ処理する\n",
        "# 評価用に全サンプルから予測と正解をリストに収集する\n",
        "references = []\n",
        "predictions = []\n",
        "for example in task.dataset:\n",
        "    # 各サンプルから入力テキスト、視覚入力、正解回答を取得\n",
        "    input_text = task.doc_to_text(example)\n",
        "    images = task.doc_to_visual(\n",
        "        example\n",
        "    )  # images は list[Image.Image] として返される前提\n",
        "    reference = task.doc_to_answer(example)\n",
        "\n",
        "    # モデルから予測を生成\n",
        "    pred = model.generate(images, input_text)\n",
        "\n",
        "    # 予測と正解をリストに追加\n",
        "    predictions.append(pred)\n",
        "    references.append(reference)"
      ],
      "metadata": {
        "id": "fYE8FY10zXtV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. スコアラーで評価を行う\n",
        "scorer = ScorerRegistry.load_scorer(\"rougel\", ScorerConfig(docs=task.dataset))\n",
        "scores = scorer.score(references, predictions)\n",
        "result = scorer.aggregate(scores)\n",
        "print(\"Evaluation result:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veymkhJIRJg5",
        "outputId": "879bf0ae-e7db-4e95-c50e-c3f08f0371a7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation result: AggregateOutput(overall_score=np.float64(8.497517725345373), details={'rougel': np.float64(8.497517725345373)})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2rk1-XeoKH2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}